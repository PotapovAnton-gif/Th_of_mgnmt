{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача стабилизации самолета при отклонении от оси  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Условие задачи, самолет задется на высоте с произвольными, небольшими отклонениями от горизонтального курса. Требуется с помощью руля крена и руля наклона поставить самолет на горизонтальный курс. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи: нужно реализовать агента с помощью q-learning и обучить его на симуляторе полетов jsbsim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Идея: У агента будет три опции за один шаг: тангаж на заданный угол, крен на заданный угол, или все вместе.\n",
    "\n",
    "Считываем показатели углов самолета, я брал так называемые pitch и roll, в нашей задаче они однозначно определяют ровно летит самолет, или нет(параллельно земле).\n",
    "Из таблицы доступных углов, выбираем два интервала куда попали углы, и определяем его состояние, действие я тоже решил определять из углов, так как именно углы в данной задаче проще всего выбрать в качестве параметров и опираться на них, но связь между состоянием и действием определяется через таблицу значений. \n",
    "\n",
    "Теперь к формулам. \n",
    "    \n",
    "    Модель:\n",
    "\n",
    "$$Q_{\\theta}(s, a) \\leftarrow Q_{{\\theta}}(s, a) + \\alpha \\cdot (r(s,a) + \\gamma \\cdot \\max_{a_n}{Q_{\\theta}(s_n,a_n)} - Q_{\\theta}(s, a))$$\n",
    "\n",
    "    Подкрепление:\n",
    "\n",
    "$$r=(2 - (\\frac{|\\phi|}{180}+\\frac{|\\theta|}{180}))^2$$ \n",
    "\n",
    "    со следующим пенальти:\n",
    "\n",
    "$$ r = r * 0.1 ~~if |\\theta|, |\\phi| > 40 \\newline r = r * 0.25 ~~if |\\theta|, |\\phi| \\in [20,40] \\newline r = r * 0.5  ~~if |\\theta|, |\\phi| \\in [10,20] \\newline r = r * 0.75~~  if |\\theta|, |\\phi| \\in [5,10] \\newline r = r * 0.9  ~~if |\\theta|, |\\phi| \\in [1,5]  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной задаче будем использовать Q-learning. Используя функции вознограждения он обучается и в конце представляетс наилучшие действия согласно модели.\n",
    " \n",
    "По сути, Q-learning позволяет агенту использовать вознаграждения среды, чтобы со временем научиться наилучшим действиям в данном состоянии.\n",
    "\n",
    "Для нашей задачи вознаграждения дается настолько больше, насколько близко агент сумеет привести самолет к нулевым Roll и Pitch\n",
    "\n",
    "Значения запомненного вознаграждения, будем хранить в Q-таблице - словарем состояние+действие = значение, тем самым происходит обучение.\n",
    "\n",
    "Q-значения инициализируются произвольным значением, и по мере того, как агент подвергается воздействию окружающей среды и получает различные вознаграждения за выполнение различных действий, Q-значения обновляются с использованием уравнения:\n",
    "\n",
    "$$ Q(s, a) \\leftarrow (1 - \\alpha)Q(s, a) + \\alpha(r + \\gamma \\max\\limits_{a} Q(s_n, n_a) $$\n",
    "\n",
    "Где:\n",
    "\n",
    "- $\\alpha$ - это скорость обучения ($0 < \\alpha \\leq 1$) - Так же, как и в условиях контролируемого обучения, это степень, с которой наши значения Q обновляются на каждой итерации.\n",
    "\n",
    "- $\\gamma$ - коэффициент скидки ($ 0 \\leq \\gamma \\leq 1$) - определяет, затухание вознаграждения, чем больше итераций, тем меньше награда за верное решение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsbsim\n",
    "import numpy as np\n",
    "from math import pi\n",
    "\n",
    "# управляет действиями\n",
    "class Actions:\n",
    "    def __init__(self) -> None:\n",
    "        self.table = {\"sustation\" : [i for i in range(169)], \"position\":[]}\n",
    "        for i in range(13):\n",
    "            for j in range(13):\n",
    "                self.table[\"position\"].append((i,j))\n",
    "        # массивы с помощью которых определяется действия значение по двум полученым углам\n",
    "        self.avalible_segments = [-180, -75, -35, -15, -5, -2,-1, 1, 2, 5, 15, 35, 75, 180]\n",
    "        self.avalible_segments2 = [-1, -2, -5, -10, -15, -25, -50, -180]\n",
    "        self.actions = [0.025, 0.05, 0.1, 0.25, 0.33, 0.5, 0.66, 0.75]\n",
    "        self.fdm = jsbsim.FGFDMExec(None)\n",
    "        self.fdm.disable_output()\n",
    "        self.good_steps_counter = 0\n",
    "\n",
    "    def Reset(self):\n",
    "        self.fdm = jsbsim.FGFDMExec(None)\n",
    "        self.fdm.disable_output()\n",
    "        self.fdm.load_script(\"scripts/c172_cruise_8K.xml\", False)\n",
    "        self.fdm[\"ic/theta-deg\"] = (np.random.random() * (2) - 1) * 20\n",
    "        self.fdm[\"ic/phi-deg\"] = (np.random.random() * (2) - 1) * 20\n",
    "        self.fdm.run_ic()\n",
    "        return self.Get_state()\n",
    "\n",
    "    def change_roll(self, angle:float):\n",
    "        self.fdm[\"fcs/aileron-cmd-norm\"] = angle\n",
    "\n",
    "    def change_pitch(self, angle:float):\n",
    "        self.fdm[\"fcs/elevator-cmd-norm\"] = angle\n",
    "\n",
    "    def Get_roll(self):\n",
    "        return self.fdm[\"attitude/roll-rad\"] * 180*2/pi\n",
    "    \n",
    "    def Get_pitch(self):\n",
    "        return self.fdm[\"attitude/pitch-rad\"] * 180*2/pi\n",
    "\n",
    "    def _get_bin(self, angle:float) -> int:\n",
    "        for i in range(len(self.avalible_segments) - 1):\n",
    "            if self.avalible_segments[i] <= angle < self.avalible_segments[i+1]:\n",
    "                return i\n",
    "        return 12\n",
    "    def Get_state(self):\n",
    "        roll = self.Get_roll()\n",
    "        pitch = self.Get_pitch()\n",
    "        return self.table[\"sustation\"][(self._get_bin(roll) * 13 + self._get_bin(pitch))]\n",
    "\n",
    "    def Get_actions(self) -> list:\n",
    "        roll = self.Get_roll()\n",
    "        pitch = self.Get_pitch()\n",
    "        sign = (lambda a: 1 if a > 0 else -1)\n",
    "        for i in range(len(self.avalible_segments2)):\n",
    "            if abs(roll) < abs(self.avalible_segments2[i]):\n",
    "                roll_actions = self.actions[i] *  sign(roll) \n",
    "                break\n",
    "\n",
    "        for i in range(len(self.avalible_segments2)):\n",
    "            if abs(pitch) < abs(self.avalible_segments2[i]):\n",
    "                pitch_actions = self.actions[i] * sign(pitch)\n",
    "                break\n",
    "        \n",
    "        return [(\"roll\", -roll_actions),(\"pitch\", pitch_actions), (\"double\", -roll_actions, pitch_actions)] \n",
    "\n",
    "\n",
    "    def reward(self):\n",
    "        roll = self.Get_roll()\n",
    "        pitch = self.Get_pitch()\n",
    "        r = (2 - (abs(roll)/180 + abs(pitch)/180)/2) ** 2\n",
    "        if abs(roll) > 40 or abs(pitch) > 40:\n",
    "            return r * 0.1\n",
    "        elif abs(roll) > 20 or abs(pitch) > 20:\n",
    "            return r * 0.25\n",
    "        elif abs(roll) > 10 or abs(pitch) > 10:\n",
    "            return r * 0.5\n",
    "        elif abs(roll) > 5 or abs(pitch) > 5:\n",
    "            return r * 0.75\n",
    "        elif abs(roll) > 1 or abs(pitch) > 1:\n",
    "            return r * 0.9\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def Step(self, tup):\n",
    "        if tup[0] == \"double\":\n",
    "            self.change_roll(tup[1])\n",
    "            self.change_pitch(tup[2])\n",
    "        elif tup[0] == \"roll\":\n",
    "            self.change_roll(tup[1])\n",
    "        else:\n",
    "            self.change_pitch(tup[1])\n",
    "        reward = self.reward()\n",
    "        done = False\n",
    "        if self.Get_roll() > 1 or self.Get_roll() < -1 or self.Get_pitch() > 1 or self.Get_pitch() < -1:\n",
    "            self.good_steps_counter = 0\n",
    "        else:\n",
    "            self.good_steps_counter += 1\n",
    "\n",
    "        if self.good_steps_counter == 10:\n",
    "            done = True\n",
    "        # done = (lambda Roll, Pitch: True if -1 < Roll and Roll > 1\\\n",
    "        #     and -1 < Pitch and Pitch > 1 else False) (self.Get_roll(), self.Get_pitch())\n",
    "        return self.Get_state(), reward, done\n",
    "    \n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300 [00:00<?, ?it/s]\n",
      "In file /home/anton/anaconda3/lib/python3.9/site-packages/jsbsim/systems/GNCUtilities.xml: line 68\n",
      "      Property navigation/actual-heading-rad is already defined.\n",
      "  0%|          | 0/300 [00:00<?, ?it/s]This vehicle has 6 bogeys, but the current \n",
      "version of FlightGear's FGNetFDM only supports 3 bogeys.\n",
      "Only the first 3 bogeys will be used.\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'roll_actions' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 91\u001b[0m\n\u001b[1;32m     89\u001b[0m acts \u001b[39m=\u001b[39m Actions()\n\u001b[1;32m     90\u001b[0m qlearn \u001b[39m=\u001b[39m QLearningAgent(\u001b[39m0.06\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m0.95\u001b[39m, acts)\n\u001b[0;32m---> 91\u001b[0m avg_rewards, best_avg_reward \u001b[39m=\u001b[39m interact(acts, qlearn)\n",
      "Cell \u001b[0;32mIn [7], line 67\u001b[0m, in \u001b[0;36minteract\u001b[0;34m(acts, agent, num_episodes)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mwhile\u001b[39;00m acts\u001b[39m.\u001b[39mfdm\u001b[39m.\u001b[39mrun():\n\u001b[1;32m     66\u001b[0m     counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m    \n\u001b[0;32m---> 67\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mGet_action()\n\u001b[1;32m     68\u001b[0m     next_state, reward, done \u001b[39m=\u001b[39m acts\u001b[39m.\u001b[39mStep(action)\n\u001b[1;32m     69\u001b[0m     agent\u001b[39m.\u001b[39mUpdate(next_state, action, reward)\n",
      "Cell \u001b[0;32mIn [7], line 36\u001b[0m, in \u001b[0;36mQLearningAgent.Get_action\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mGet_action\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m---> 36\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions\u001b[39m.\u001b[39;49mGet_actions()\n\u001b[1;32m     37\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandom() \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepsilon:\n\u001b[1;32m     38\u001b[0m         action \u001b[39m=\u001b[39m actions[np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(actions))], \u001b[39m1\u001b[39m)[\u001b[39m0\u001b[39m]]\n",
      "Cell \u001b[0;32mIn [6], line 65\u001b[0m, in \u001b[0;36mActions.Get_actions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m         pitch_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactions[i] \u001b[39m*\u001b[39m sign(pitch)\n\u001b[1;32m     63\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m [(\u001b[39m\"\u001b[39m\u001b[39mroll\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39mroll_actions),(\u001b[39m\"\u001b[39m\u001b[39mpitch\u001b[39m\u001b[39m\"\u001b[39m, pitch_actions), (\u001b[39m\"\u001b[39m\u001b[39mdouble\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m-\u001b[39mroll_actions, pitch_actions)]\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'roll_actions' referenced before assignment"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class QLearningAgent:\n",
    "   \n",
    "    def __init__(self, alpha, epsilon, gamma, actions:Actions) -> None:\n",
    "        self._q_values = defaultdict(lambda: defaultdict(lambda : 0))\n",
    "        self.alpha = alpha # lr = 0.01\n",
    "        self.gamma = gamma # gamma = 0.95\n",
    "        self.epsilon = epsilon #eps = 0.5\n",
    "        self.actions = actions\n",
    "\n",
    "    def Set_q_value(self, state, action, value):\n",
    "        self._q_values[state][action] = value\n",
    "\n",
    "    def Get_q_value(self, state, action):\n",
    "        return self._q_values[state][action]\n",
    "\n",
    "    def Get_value(self):\n",
    "        actions = self.actions.Get_actions()\n",
    "        state = self.actions.Get_state()\n",
    "        value = max([self.Get_q_value(state, action) for action in actions])\n",
    "        return value\n",
    "\n",
    "    def Policy(self):\n",
    "        state = self.actions.Get_state()\n",
    "        actions = self.actions.Get_actions()\n",
    "        best_action = actions[0]\n",
    "        for action in actions:\n",
    "            if self.Get_q_value(state, action) > self.Get_q_value(state, best_action):\n",
    "                best_action = action\n",
    "        return best_action\n",
    "            \n",
    "    def Get_action(self):\n",
    "        actions = self.actions.Get_actions()\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action = actions[np.random.choice([i for i in range(len(actions))], 1)[0]]\n",
    "        else:\n",
    "            action = self.Policy()\n",
    "        return action\n",
    "    \n",
    "    def Update(self, state, action, reward):\n",
    "        self.epsilon -= 0.00001\n",
    "        addition = self.alpha*(reward + self.gamma *(self.Get_value()-self.Get_q_value(state, action)))\n",
    "        reference_value = self.Get_q_value(state, action) + addition\n",
    "        self.Set_q_value(state, action, reference_value)\n",
    "\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "\n",
    "def interact(acts:Actions, agent:QLearningAgent, num_episodes=300):\n",
    "    average_reward_per_100_episodes = []\n",
    "    best_average_reward_per_100_episodes = []\n",
    "    avg_rewards = deque(maxlen=num_episodes)\n",
    "    best_avg_reward = -math.inf\n",
    "    rewards = deque(maxlen=5000)\n",
    "\n",
    "    for episode in tqdm(range(num_episodes)):\n",
    "        state = acts.Reset()\n",
    "        \n",
    "        counter = 0\n",
    "        curr_reward = 0\n",
    "        agent.epsilon = 1\n",
    "        while acts.fdm.run():\n",
    "            counter += 1    \n",
    "            action = agent.Get_action()\n",
    "            next_state, reward, done = acts.Step(action)\n",
    "            agent.Update(next_state, action, reward)\n",
    "            curr_reward += reward\n",
    "            state = next_state\n",
    "            if done:\n",
    "                rewards.append(curr_reward)\n",
    "                break\n",
    "          \n",
    "        if episode % 100 == 0:\n",
    "            avg_reward = np.mean(rewards)\n",
    "            avg_rewards.append(avg_reward)\n",
    "            print('episode average reward {}'.format(avg_reward))\n",
    "            average_reward_per_100_episodes.append(avg_reward)\n",
    "            best_average_reward_per_100_episodes.append(best_avg_reward)\n",
    "            \n",
    "            if avg_reward > best_avg_reward:\n",
    "                best_avg_reward = avg_reward\n",
    "                \n",
    "            print(\"\\rEpisode {}/{} || Best average reward {} || eps {} \".format(episode, num_episodes, best_avg_reward, agent.epsilon), end=\"\")\n",
    "    return avg_rewards, best_avg_reward\n",
    "\n",
    "acts = Actions()\n",
    "qlearn = QLearningAgent(0.06, 1, 0.95, acts)\n",
    "avg_rewards, best_avg_reward = interact(acts, qlearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "In file /home/anton/anaconda3/lib/python3.9/site-packages/jsbsim/systems/GNCUtilities.xml: line 68\n",
      "      Property navigation/actual-heading-rad is already defined.\n",
      "This vehicle has 6 bogeys, but the current \n",
      "version of FlightGear's FGNetFDM only supports 3 bogeys.\n",
      "Only the first 3 bogeys will be used.\n",
      "\n",
      "In file /home/anton/anaconda3/lib/python3.9/site-packages/jsbsim/systems/GNCUtilities.xml: line 68\n",
      "      Property navigation/actual-heading-rad is already defined.\n",
      "This vehicle has 6 bogeys, but the current \n",
      "version of FlightGear's FGNetFDM only supports 3 bogeys.\n",
      "Only the first 3 bogeys will be used.\n",
      "\n",
      "In file /home/anton/anaconda3/lib/python3.9/site-packages/jsbsim/systems/GNCUtilities.xml: line 68\n",
      "      Property navigation/actual-heading-rad is already defined.\n",
      "This vehicle has 6 bogeys, but the current \n",
      "version of FlightGear's FGNetFDM only supports 3 bogeys.\n",
      "Only the first 3 bogeys will be used.\n",
      "\n",
      "In file /home/anton/anaconda3/lib/python3.9/site-packages/jsbsim/systems/GNCUtilities.xml: line 68\n",
      "      Property navigation/actual-heading-rad is already defined.\n",
      "This vehicle has 6 bogeys, but the current \n",
      "version of FlightGear's FGNetFDM only supports 3 bogeys.\n",
      "Only the first 3 bogeys will be used.\n",
      "\n",
      "In file /home/anton/anaconda3/lib/python3.9/site-packages/jsbsim/systems/GNCUtilities.xml: line 68\n",
      "      Property navigation/actual-heading-rad is already defined.\n",
      "This vehicle has 6 bogeys, but the current \n",
      "version of FlightGear's FGNetFDM only supports 3 bogeys.\n",
      "Only the first 3 bogeys will be used.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "pitch_list = []\n",
    "roll_list = []\n",
    "for _ in range(5):\n",
    "    acts.Reset()\n",
    "    pitch = []\n",
    "    roll = []\n",
    "    while acts.fdm.run():\n",
    "        roll.append(acts.Get_roll())\n",
    "        pitch.append(acts.Get_pitch())\n",
    "        action = qlearn.Get_action()\n",
    "        next_state, reward, done = acts.Step(action)\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    roll_list.append(roll)\n",
    "    pitch_list.append(pitch)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pitch_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pitch_list \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39marray(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pitch_list]\n\u001b[1;32m      2\u001b[0m roll_list \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39marray(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m roll_list]\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pitch_list' is not defined"
     ]
    }
   ],
   "source": [
    "pitch_list = [np.array(i) for i in pitch_list]\n",
    "roll_list = [np.array(i) for i in roll_list]\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "colors = [\"green\", \"blue\", \"black\", \"red\", \"purple\"]\n",
    "\n",
    "plt.subplot(211)\n",
    "for i in pitch_list:\n",
    "    plt.plot(np.arange(len(i)), i, color=colors[i])\n",
    "plt.title(\"pitch\")\n",
    "plt.grid()\n",
    "plt.subplot(212)\n",
    "for i in roll_list:\n",
    "    plt.plot(np.arange(len(i)), i, color=colors[i])\n",
    "plt.title(\"roll\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d2ca3662093a4c1fe41efe52a3a11511f0966fe72139b928426d1812864efca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
